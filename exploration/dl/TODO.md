â— Excellent work on fixing the bug! Let me explain how the hook-based testing works.

  How PyTorch Hooks Work

  PyTorch modules have a hook system that lets you "tap into" the forward/backward pass without modifying the model code:

                      Forward Pass
                           â”‚
      Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Output
                           â”‚
                      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                      â”‚  HOOK   â”‚  â† Intercepts and records
                      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                     activations dict

  The Key Components in test_resnet.py

  1. The Hook Function (lines 22-25)
  def _make_hook(self, name: str):
      def hook(module: nn.Module, input: tuple, output: torch.Tensor):
          self.activations[name] = output.detach().clone()
      return hook

  This creates a closure that captures the layer name. When called:
  - module: The layer being hooked (e.g., conv1)
  - input: What went INTO the layer
  - output: What came OUT of the layer
  - We .detach().clone() to avoid affecting gradients and ensure we keep a copy

  2. Registering Hooks (lines 37-40)
  for name, module in model.named_modules():
      if name in layer_names:
          hook = module.register_forward_hook(self._make_hook(name))
          self._hooks.append(hook)

  named_modules() gives us every submodule with its name:
  '' -> BasicBlock (the root)
  'conv1' -> Conv2d
  'bn1' -> BatchNorm2d
  'relu' -> ReLU
  ...

  3. Cleanup with Context Manager (lines 42-47)
  try:
      yield self
  finally:
      for hook in self._hooks:
          hook.remove()  # Critical! Prevents memory leaks

  Hooks persist until removed. The context manager ensures cleanup even if an exception occurs.

  ---
  â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Why hooks matter beyond testing:
  - Feature extraction: Grab intermediate features for transfer learning
  - Interpretability: Visualize what each layer "sees" (GradCAM, etc.)
  - Debugging: Find where NaN/Inf values first appear
  - Profiling: Measure time/memory per layer

  The same pattern is used in production ML systems!
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ---
  Your Turn: Implement Two Tests

  I'll add test stubs for you to implement. These will deepen your understanding:

â— Read(exploration/dl/test_resnet.py)
  â¿  Read 243 lines

â— Update(exploration/dl/test_resnet.py)
  â¿  Updated exploration/dl/test_resnet.py with 64 additions                                   
       218        print("\n" + "=" * 60)
       219    
       220    
       221 +  def test_residual_addition_happens():
       222 +      """
       223 +      Verify that the skip connection actually ADDS the identity to the output.
       224 +  
       225 +      This is a mathematical verification: if we zero out the conv weights,
       226 +      the output should be relu(identity) since conv outputs will be ~zero.
       227 +  
       228 +      TODO(human): Implement this test
       229 +  
       230 +      Steps:
       231 +      1. Create a BasicBlock with in_channels=64, planes=64, stride=1
       232 +      2. Use torch.no_grad() context and zero out all conv weights:
       233 +         - block.conv1.weight.zero_()
       234 +         - block.conv2.weight.zero_()
       235 +      3. Create a known input, e.g., torch.ones(1, 64, 8, 8) * 0.5
       236 +      4. Run the forward pass
       237 +      5. Since convs output ~0 after BN, output â‰ˆ relu(identity) = relu(input)
       238 +      6. Assert that output values match relu(input) approximately
       239 +         Hint: use torch.allclose(output, expected, atol=1e-5)
       240 +  
       241 +      Why this matters: This proves the skip connection is actually adding,
       242 +      not just passing through one path or the other.
       243 +      """
       244 +      from resnet import BasicBlock
       245 +  
       246 +      print("=" * 60)
       247 +      print("TEST: Residual Addition Verification")
       248 +      print("=" * 60)
       249 +  
       250 +      # TODO(human): Implement the test here
       251 +      pass
       252 +  
       253 +  
       254 +  def test_batchnorm_train_vs_eval():
       255 +      """
       256 +      Verify that BatchNorm behaves differently in train vs eval mode.
       257 +  
       258 +      In train mode: BN uses batch statistics (mean/var of current batch)
       259 +      In eval mode: BN uses running statistics (accumulated during training)
       260 +  
       261 +      TODO(human): Implement this test
       262 +  
       263 +      Steps:
       264 +      1. Create a BasicBlock
       265 +      2. Create an input tensor x = torch.randn(4, 64, 8, 8)  # batch_size=4
       266 +      3. Run forward in train mode: block.train(); out_train = block(x)
       267 +      4. Run forward in eval mode: block.eval(); out_eval = block(x)
       268 +      5. Assert that outputs are DIFFERENT: not torch.allclose(out_train, out_eval)
       269 +  
       270 +      Why this matters: Understanding train/eval mode is critical for:
       271 +      - Getting correct inference results
       272 +      - Debugging "works in training, fails in production" issues
       273 +      - Understanding why model.eval() is required before inference
       274 +      """
       275 +      from resnet import BasicBlock
       276 +  
       277 +      print("=" * 60)
       278 +      print("TEST: BatchNorm Train vs Eval Mode")
       279 +      print("=" * 60)
       280 +  
       281 +      # TODO(human): Implement the test here
       282 +      pass
       283 +  
       284 +  
       285    if __name__ == "__main__":
       286        print("\nğŸ§ª Running BasicBlock Tests\n")
       287    

â— ---
  â— Learn by Doing

  Context: I've added two test stubs to test_resnet.py. These tests verify deeper properties of ResNet beyond just shapes - they prove the
   mathematical behavior is correct.

  Your Task: In exploration/dl/test_resnet.py, implement both test functions. Look for TODO(human) in:
  1. test_residual_addition_happens() (line 250) - proves the skip connection adds
  2. test_batchnorm_train_vs_eval() (line 281) - shows train/eval mode differences

  Guidance:

  For test 1 (residual addition):
  - When conv weights are zero, the main path outputs ~0 (after BN normalizes)
  - So output = relu(0 + identity) = relu(identity)
  - Use block.eval() for deterministic BN, and torch.no_grad() when zeroing weights

  For test 2 (train vs eval):
  - Fresh BN layers have running_mean=0 and running_var=1
  - In train mode, BN uses the batch's actual mean/var
  - In eval mode, BN uses the running mean/var (which differ for random input)
  - Make sure to use the same input tensor for both passes!

  Run with: uv run pytest exploration/dl/test_resnet.py -v -k "residual or batchnorm"