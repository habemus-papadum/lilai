{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ba4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc7f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from transformers import AutoModelForCausalLM, AutoModelForAudioClassification\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d5697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "\n",
    "# --- Helper: standard profiling function ---\n",
    "def profile_model(model, inputs, model_name):\n",
    "    print(f\"\\n--- Profiling {model_name} ---\")\n",
    "    model.to(DEVICE)\n",
    "    model.eval() # Inference mode\n",
    "    \n",
    "    # Ensure inputs are on the correct device\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    else:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "\n",
    "    # 2.9 Best Practice: Use a schedule to skip noise and warmup\n",
    "    # wait=1 (skip first step), warmup=1 (compile/warmup), active=3 (record 3 steps)\n",
    "    my_schedule = torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA] if DEVICE == \"cuda\" else [ProfilerActivity.CPU],\n",
    "        schedule=my_schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./log/{model_name}'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as p:\n",
    "        \n",
    "        # We simulate a loop of 5 steps (1 wait + 1 warmup + 3 active)\n",
    "        for step in range(5):\n",
    "            with record_function(f\"inference_step_{step}\"):\n",
    "                if isinstance(inputs, dict):\n",
    "                    _ = model(**inputs)\n",
    "                else:\n",
    "                    _ = model(inputs)\n",
    "            p.step() # Signal to profiler that a step is done\n",
    "\n",
    "    print(f\"Profiling complete. Logs saved to ./log/{model_name}\")\n",
    "    # Print a quick text summary of the *active* steps\n",
    "    print(p.key_averages().table(sort_by=\"cuda_time_total\" if DEVICE == \"cuda\" else \"cpu_time_total\", row_limit=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840af10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vision Model...\n",
      "\n",
      "--- Profiling vision_resnet50 ---\n",
      "Profiling complete. Logs saved to ./log/vision_resnet50\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.34%     173.453us        48.53%      25.090ms       8.363ms       0.000us         0.00%      50.367ms      16.789ms           0 B           0 B     880.00 KB           0 B             3  \n",
      "                                           aten::conv2d         0.71%     369.553us        16.27%       8.414ms      52.921us       0.000us         0.00%      22.329ms     140.437us           0 B           0 B      12.45 GB           0 B           159  \n",
      "                                      aten::convolution         1.54%     795.472us        15.56%       8.045ms      50.596us       0.000us         0.00%      22.329ms     140.437us           0 B           0 B      12.45 GB           0 B           159  \n",
      "                                     aten::_convolution         1.07%     553.577us        14.02%       7.249ms      45.593us       0.000us         0.00%      22.329ms     140.437us           0 B           0 B      12.45 GB           0 B           159  \n",
      "                                aten::cudnn_convolution         7.60%       3.931ms        12.95%       6.696ms      42.112us      22.329ms        44.18%      22.329ms     140.437us           0 B           0 B      12.45 GB      12.45 GB           159  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 51.703ms\n",
      "Self CUDA time total: 50.538ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Vision Model: ResNet50 (Standard, Easy to load) ---\n",
    "# We use torchvision's new weight enum standard\n",
    "print(\"Loading Vision Model...\")\n",
    "vision_weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "vision_model = torchvision.models.resnet50(weights=vision_weights)\n",
    "vision_inputs = torch.randn(100, 3, 224, 224) # Standard ImageNet batch\n",
    "profile_model(vision_model, vision_inputs, \"vision_resnet50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90de6e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60211200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * 3 * 224 * 224 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff406ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "\n",
      "--- Profiling llm_gpt2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W109 20:20:02.701987353 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling complete. Logs saved to ./log/llm_gpt2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.10%     197.338us        78.53%     159.293ms      53.098ms       0.000us         0.00%     161.728ms      53.909ms         480 B           0 B           0 B           0 B             3  \n",
      "                                            aten::addmm         1.67%       3.388ms         2.11%       4.284ms      29.753us      91.498ms        45.52%      91.498ms     635.400us           0 B           0 B       7.59 GB       7.59 GB           144  \n",
      "void cutlass::Kernel2<cutlass_80_simt_sgemm_256x128_...         0.00%       0.000us         0.00%       0.000us       0.000us      73.884ms        36.76%      73.884ms     839.586us           0 B           0 B           0 B           0 B            88  \n",
      "                                       inference_step_2         0.00%       0.000us         0.00%       0.000us       0.000us      54.806ms        27.27%      54.806ms      54.806ms           0 B           0 B           0 B           0 B             1  \n",
      "                                       inference_step_4         0.00%       0.000us         0.00%       0.000us       0.000us      54.497ms        27.11%      54.497ms      54.497ms           0 B           0 B           0 B           0 B             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 202.843ms\n",
      "Self CUDA time total: 201.003ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. LLM: GPT-2 (Small, Open Weights, Standard Architecture) ---\n",
    "# We use HuggingFace transformers. GPT-2 is chosen for speed/simplicity over Llama 3 for a basic script.\n",
    "print(\"Loading LLM...\")\n",
    "llm_name = \"gpt2\" \n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_name)\n",
    "\n",
    "B=64\n",
    "L=128\n",
    "llm_inputs = {\"input_ids\": torch.randint(0, 50257, (B, L)), \"attention_mask\": torch.ones(B, L)}\n",
    "profile_model(llm_model, llm_inputs, \"llm_gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f3b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Audio Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51293afe40ff4389a2916dec678bfd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7e64edaacd41e2a3a870140c9d35e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Profiling audio_wav2vec2 ---\n",
      "Profiling complete. Logs saved to ./log/audio_wav2vec2\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.49%     162.012us        99.97%      33.211ms      11.070ms       0.000us         0.00%      15.669ms       5.223ms         192 B           0 B     642.00 KB           0 B             3  \n",
      "                                       inference_step_2         0.00%       0.000us         0.00%       0.000us       0.000us      12.995ms        82.94%      12.995ms       1.444ms           0 B           0 B           0 B           0 B             9  \n",
      "                                       inference_step_4         0.00%       0.000us         0.00%       0.000us       0.000us      11.857ms        75.67%      11.857ms       1.317ms           0 B           0 B           0 B           0 B             9  \n",
      "                                       inference_step_3         0.00%       0.000us         0.00%       0.000us       0.000us      11.764ms        75.08%      11.764ms       1.307ms           0 B           0 B           0 B           0 B             9  \n",
      "                                           aten::conv1d         0.19%      63.720us         7.93%       2.635ms     109.799us       0.000us         0.00%       7.273ms     303.041us           0 B           0 B      38.92 MB           0 B            24  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 33.222ms\n",
      "Self CUDA time total: 15.669ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Audio Model: Wav2Vec2 (Standard Speech Architecture) ---\n",
    "# Using a small classifier version for speed\n",
    "print(\"Loading Audio Model...\")\n",
    "audio_name = \"facebook/wav2vec2-base-960h\"\n",
    "audio_model = AutoModelForAudioClassification.from_pretrained(audio_name)\n",
    "# Dummy audio waveform: Batch 1, 16000 samples (1 second of audio at 16kHz)\n",
    "audio_inputs = {\"input_values\": torch.randn(1, 16000)} \n",
    "profile_model(audio_model, audio_inputs, \"audio_wav2vec2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d62f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lilai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
